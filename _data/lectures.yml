- date: 1/21
  lecturer:
  title: >
    <strong>Introduction</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_1.pdf"
  readings:
    - No associated readings
  logistics:

- title: "Language Modeling"

- date: 1/23
  lecturer:
  title: >
    <strong>Language modeling</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_2.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Jurafsky and Martin, Chapter 3.1-3.5</a>
  logistics: Homework 0 released on Piazza (due 2/7)

- date: 1/28
  lecturer:
  title: >
    <strong>Neural language models</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_3.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf" target="_blank">Jurafsky and Martin, Chapter 7.1-7.4 and 7.6</a>
    - Bengio et al. (2003) <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank">A Neural Probabilistic Language Model</a>
  logistics:

- date: 1/30
  lecturer:
  title: >
    <strong>Backpropagation</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_4.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf" target="_blank">Jurafsky and Martin, Chapter 7.5 and 7.7</a>
  logistics: Quiz 0 released on Piazza (due 2/7)

- date: 2/4
  title: Class canceled because Tu was sick
  
- date: 2/6
  lecturer:
  title: >
    <strong>Word Embeddings</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_5.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Jurafsky and Martin, Chapter 6</a>
    - Mikolov et al. (2013a) <a href="https://arxiv.org/pdf/1310.4546" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a>
    - Mikolov et al. (2013b) <a href="https://arxiv.org/pdf/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>
    - "[optional] Pennington et al. (2014) <a href='https://nlp.stanford.edu/pubs/glove.pdf' target='_blank'>GloVe: Global Vectors for Word Representation</a>"
  logistics:

- title: "Transformers and the Evolution of LLMs"

- date: 2/11
  title: Class canceled due to inclement weather

- date: 2/13
  lecturer:
  title: >
    <strong>Transformers</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_6.pdf"
  readings:
  logistics:

- date: 2/18
  lecturer:
  title: >
    <strong>The Era of BERT</strong>
  readings:
    - "Devlin et al. (2018) <a href='https://arxiv.org/pdf/1810.04805' target='_blank'>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>"
    - Raffel et al. (2019) <a href="https://arxiv.org/pdf/1910.10683" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>
  logistics:

- date: 2/20
  lecturer:
  title: >
    <strong>Scaling LLM Pretraining</strong>
  readings:
    - Kaplan et al. (2020) <a href="https://arxiv.org/pdf/2001.08361" target="_blank">Scaling Laws for Neural Language Models</a>
    - Hoffmann et al. (2022) <a href="https://arxiv.org/pdf/2203.15556" target="_blank">Training Compute-Optimal Large Language Models</a>
  logistics:

- title: "LLM Capabilities and Evaluation"

- date: 2/25
  lecturer:
  title: >
    <strong>LLM Prompting</strong>
  readings:
    - Brown et al. (2020) <a href="https://arxiv.org/pdf/2005.14165" target="_blank">Language Models are Few-Shot Learners</a>
    - Wei et al. (2022) <a href="https://arxiv.org/pdf/2201.11903" target="_blank">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>
  logistics:

- date: 2/27
  lecturer:
  title: >
    <strong>LLM Decoding</strong>
  readings:
    - Holtzman et al. (2019) <a href="https://arxiv.org/pdf/1904.09751" target="_blank">The Curious Case of Neural Text Degeneration</a>
  logistics:

- date: 3/4
  lecturer:
  title: >
    <strong>Instruction tuning</strong>
  readings:
    - Wei et al. (2021) <a href="https://arxiv.org/pdf/2109.01652" target="_blank">Finetuned Language Models Are Zero-Shot Learners</a>
    - Chung et al. (2022) <a href="https://arxiv.org/pdf/2210.11416" target="_blank">Scaling Instruction-Finetuned Language Models</a>
  logistics:

- date: 3/6
  lecturer:
  title: >
    <strong>LLM Alignment</strong>
  readings:
    - Ouyang et al. (2022) <a href="https://arxiv.org/pdf/2203.02155" target="_blank">Training language models to follow instructions with human feedback</a>
    - "Rafailov et al. (2023) <a href='https://arxiv.org/pdf/2305.18290' target='_blank'>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>"
  logistics:

- date: 3/11
  title: No classes (Spring break)

- date: 3/13
  title: No classes (Spring break)

- date: 3/18
  lecturer:
  title: >
    <strong>LLM Evaluation</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- title: "Improving LLM Efficiency and Adaptability"

- date: 3/20
  lecturer:
  title: >
    <strong>Parameter-efficient fine-tuning</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 3/25
  lecturer:
  title: >
    <strong>Mixture of Experts</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 3/27
  lecturer:
  title: >
    <strong>Model Merging</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/1
  lecturer:
  title: >
    <strong>Distillation, quantization, and pruning</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/3
  lecturer:
  title: >
    <strong>Long-context LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- title: "Advanced LLMs and Compound AI Systems"

- date: 4/8
  lecturer:
  title: >
    <strong>Thinking LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/10
  lecturer:
  title: >
    <strong>Scaling test-time compute</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/15
  lecturer:
  title: >
    <strong>Retrieval-augmented generation (RAG) & Tool-use LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/17
  lecturer:
  title: >
    <strong>LLM Agents</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- title: "Other topics"

- date: 4/22
  lecturer:
  title: >
    <strong>Multimodal LLMs & Multilingual LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/24
  lecturer:
  title: >
    <strong>Code and Math LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/29
  lecturer:
  title: >
    <strong>Token-free LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 5/1
  lecturer:
  title: >
    <strong>LLM Safety and Security</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 5/6
  lecturer:
  title: >
    <strong>State Space Models (SSM)</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 5/14
  title: "Project presentations (Time & Location: TBD)"
