- date: 1/21
  lecturer:
  title: >
    <strong>Introduction</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_1.pdf"
  readings:
    - No associated readings
  logistics:

- title: "Language Modeling"

- date: 1/23
  lecturer:
  title: >
    <strong>Language modeling</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_2.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Jurafsky and Martin, Chapter 3.1-3.5</a>
  logistics: Homework 0 released on Piazza (due 2/7)

- date: 1/28
  lecturer:
  title: >
    <strong>Neural language models</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_3.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf" target="_blank">Jurafsky and Martin, Chapter 7.1-7.4 and 7.6</a>
    - Bengio et al. (2003) <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank">A Neural Probabilistic Language Model</a>
  logistics:

- date: 1/30
  lecturer:
  title: >
    <strong>Backpropagation</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_4.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf" target="_blank">Jurafsky and Martin, Chapter 7.5 and 7.7</a>
  logistics: Quiz 0 released on Piazza (due 2/7)

- date: 2/4
  title: Class canceled because Tu was sick
  
- date: 2/6
  lecturer:
  title: >
    <strong>Word Embeddings</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_5.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Jurafsky and Martin, Chapter 6</a>
    - Mikolov et al. (2013a) <a href="https://arxiv.org/pdf/1310.4546" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a>
    - Mikolov et al. (2013b) <a href="https://arxiv.org/pdf/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>
    - "[optional] Pennington et al. (2014) <a href='https://nlp.stanford.edu/pubs/glove.pdf' target='_blank'>GloVe: Global Vectors for Word Representation</a>"
  logistics:

- title: "Transformers and the Evolution of LLMs"

- date: 2/11
  title: Class canceled due to inclement weather

- date: 2/13
  lecturer:
  title: >
    <strong>Transformers</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_6.pdf"
  readings:
    - Bahdanau et al. (2014) <a href="https://arxiv.org/pdf/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a>
    - Vaswani et al. (2017) <a href="https://arxiv.org/pdf/1706.03762" target="_blank">Attention Is All You Need</a>
    - Jay Alammar's blog <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a>
    - Jay Alammar's blog <a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a>
  logistics:

- date: 2/18
  lecturer:
  title: >
    <strong>The Era of BERT</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_7.pdf"
  readings:
    - "Devlin et al. (2018) <a href='https://arxiv.org/pdf/1810.04805' target='_blank'>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>"
    - Raffel et al. (2019) <a href="https://arxiv.org/pdf/1910.10683" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>
  logistics:

- date: 2/20
  lecturer:
  title: >
    <strong>Scaling LLM Pretraining</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_8.pdf"
  readings:
    - Kaplan et al. (2020) <a href="https://arxiv.org/pdf/2001.08361" target="_blank">Scaling Laws for Neural Language Models</a>
    - Hoffmann et al. (2022) <a href="https://arxiv.org/pdf/2203.15556" target="_blank">Training Compute-Optimal Large Language Models</a>
    - "Li et al. (2025) <a href='https://arxiv.org/pdf/2502.18969' target='_blank'>(Mis)Fitting: A Survey of Scaling Laws</a>"
  logistics:

- title: "LLM Capabilities and Evaluation"

- date: 2/25
  lecturer:
  title: >
    <strong>LLM Prompting</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_9.pdf"
  readings:
    - Brown et al. (2020) <a href="https://arxiv.org/pdf/2005.14165" target="_blank">Language Models are Few-Shot Learners</a>
    - Wei et al. (2022) <a href="https://arxiv.org/pdf/2201.11903" target="_blank">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>
  logistics:

- date: 2/27
  lecturer:
  title: >
    <strong>LLM Decoding</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_10.pdf"
  readings:
    - Holtzman et al. (2019) <a href="https://arxiv.org/pdf/1904.09751" target="_blank">The Curious Case of Neural Text Degeneration</a>
  logistics:

- date: 3/4
  lecturer:
  title: >
    <strong>Instruction tuning</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_11.pdf"
  readings:
    - Wei et al. (2021) <a href="https://arxiv.org/pdf/2109.01652" target="_blank">Finetuned Language Models Are Zero-Shot Learners</a>
    - Chung et al. (2022) <a href="https://arxiv.org/pdf/2210.11416" target="_blank">Scaling Instruction-Finetuned Language Models</a>
    - "[optional] Sanh et al. (2021) <a href='https://arxiv.org/pdf/2110.08207' target='_blank'>Multitask Prompted Training Enables Zero-Shot Task Generalization</a>"
    - "[optional] Longpre et al. (2023) <a href='https://arxiv.org/pdf/2301.13688' target='_blank'>The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</a>"
  logistics:

- date: 3/6
  lecturer:
  title: >
    <strong>LLM Alignment</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_12.pdf"
  readings:
    - Ouyang et al. (2022) <a href="https://arxiv.org/pdf/2203.02155" target="_blank">Training language models to follow instructions with human feedback</a>
    - "Rafailov et al. (2023) <a href='https://arxiv.org/pdf/2305.18290' target='_blank'>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>"
  logistics:

- date: 3/11
  title: No classes (Spring break)

- date: 3/13
  title: No classes (Spring break)

- date: 3/18
  lecturer:
  title: >
    <strong>LLM Evaluation</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_13.pdf"
  readings:
    - Zheng et al. (2023) <a href="https://arxiv.org/pdf/2306.05685" target="_blank">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a>
    - "[optional] Vu et al. (2024) <a href='https://arxiv.org/pdf/2407.10817' target='_blank'>Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</a>"
  logistics:

- title: "Improving LLM Efficiency and Adaptability"

- date: 3/20
  lecturer:
  title: >
    <strong>Parameter-efficient fine-tuning</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_14.pdf"
  readings:
    - Lester et al. (2021) <a href="https://arxiv.org/pdf/2104.08691" target="_blank">The Power of Scale for Parameter-Efficient Prompt Tuning</a>
    - "Hu et al. (2021) <a href='https://arxiv.org/pdf/2106.09685' target='_blank'>LoRA: Low-Rank Adaptation of Large Language Models</a>"
    - "[optional] Vu et al. (2022) <a href='https://arxiv.org/pdf/2110.07904' target='_blank'>SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</a>"
  logistics:

- date: 3/25
  lecturer:
  title: >
    <strong>Mixture of Experts</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_15.pdf"
  readings:
    - "Fedus et al. (2021) <a href='https://arxiv.org/pdf/2101.03961' target='_blank'>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>"
    - "Shen et al. (2023) <a href='https://arxiv.org/pdf/2305.14705' target='_blank'>Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models</a>"
    - "[optional] Zoph et al. (2022) <a href='https://arxiv.org/pdf/2202.08906' target='_blank'>ST-MoE: Designing Stable and Transferable Sparse Expert Models</a>"
    - "[optional] Lepikhin et al. (2020) <a href='https://arxiv.org/pdf/2006.16668' target='_blank'>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a>"
  logistics:

- date: 3/27
  lecturer:
  title: >
    <strong>Model Merging</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_16.pdf"
  readings:
    - Ilharco et al. (2022) <a href="https://arxiv.org/pdf/2212.04089" target="_blank">Editing Models with Task Arithmetic</a>
    - "Yadav et al. (2023) <a href='https://arxiv.org/pdf/2305.14705' target='_blank'>TIES-Merging: Resolving Interference When Merging Models</a>"
  logistics:

- date: 4/1
  lecturer:
  title: >
    <strong>Distillation, quantization, and pruning</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_17.pdf"
  readings:
    - Hinton et al. (2015) <a href="https://arxiv.org/pdf/1503.02531" target="_blank">Distilling the Knowledge in a Neural Network</a>
    - Maarten Grootendorst's blog <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization" target="_blank">A Visual Guide to Quantization</a>
    - "Frankle and Carbin (2018) <a href='https://arxiv.org/pdf/1803.03635' target='_blank'>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a>"
  logistics:

- date: 4/3
  lecturer:
  title: >
    <strong>Long-context LLMs</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_18.pdf"
  readings:
    - "Dao et al. (2022) <a href='https://arxiv.org/pdf/2205.14135' target='_blank'>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>"
    - "Gao et al. (2024) <a href='https://arxiv.org/pdf/2410.02660' target='_blank'>How to Train Long-Context Language Models (Effectively)</a>"
  logistics:

- title: "Advanced LLMs and Compound AI Systems"

- date: 4/8
  lecturer:
  title: >
    <strong>Advanced reasoning & Test-time scaling</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_19.pdf"
  readings:
    - "DeepSeek-AI (2025) <a href='https://arxiv.org/pdf/2501.19393' target='_blank'>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a>"
    - "Muennighoff et al. (2025) <a href='https://arxiv.org/pdf/2501.19393' target='_blank'>s1: Simple test-time scaling</a>"
    - "Brown et al. (2024) <a href='https://arxiv.org/pdf/2407.21787' target='_blank'>Large language monkeys: Scaling inference compute with repeated sampling</a>"
    - "[optional] Geiping et al. (2025) <a href='https://www.arxiv.org/pdf/2502.05171' target='_blank'>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</a>"
  logistics:

- date: 4/10
  lecturer:
  title: >
    <strong>Advanced reasoning & Test-time scaling (cont'd)</strong>
  slides: "https://tuvllms.github.io/nlp-spring-2025/assets/pdf/lecture_20.pdf"
  readings:
    - "Ye et al. (2025) <a href='https://arxiv.org/pdf/2502.03387' target='_blank'>LIMO: Less is More for Reasoning</a>"
    - "Yu et al. (2025) <a href='https://arxiv.org/pdf/2504.00810' target='_blank'>Z1: Efficient Test-time Scaling with Code</a>"
    - "[optional] Xiang et al. (2025) <a href='https://arxiv.org/pdf/2501.04682' target='_blank'>Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought</a>"
  logistics:

- date: 4/15
  lecturer:
  title: >
    <strong>Retrieval-augmented generation (RAG) & Tool-use LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/17
  lecturer:
  title: >
    <strong>LLM Agents</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- title: "Other topics"

- date: 4/22
  lecturer:
  title: >
    <strong>Multimodal LLMs & Multilingual LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/24
  lecturer:
  title: >
    <strong>Code and Math LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 4/29
  lecturer:
  title: >
    <strong>Token-free LLMs</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 5/1
  lecturer:
  title: >
    <strong>LLM Safety and Security</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 5/6
  lecturer:
  title: >
    <strong>State Space Models (SSM)</strong>
  readings:
    - <a href="" target="_blank"></a>
  logistics:

- date: 5/14
  title: "Project presentations (Time & Location: TBD)"
